# -*- coding: utf-8 -*-
"""speech_classification_bert.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wAZmaDI1FO7Gsn9CPcJNBEQTaFuAkbkM
"""

import torch
import pandas as pd
import pytorch_lightning as pl
import numpy as np
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import recall_score
import torchmetrics
from tqdm import tqdm
import pickle
import os

from tqdm import tqdm
tqdm.pandas()

class TranscriptData(torch.utils.data.Dataset):
    def __init__(self, df, le = None, target='상황', test=False):
        self.test = test
        self.df = df
        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-multilingual-cased')
        self.df['tokenized'] = self.df.progress_apply(lambda x: self.tokenizer(x['발화문'], padding='max_length', truncation=True, max_length=512, return_tensors='pt'), axis=1)

        #Squeeze the tensors
        self.df['tokenized'] = self.df.progress_apply(lambda x: {k: v.squeeze() for k, v in x['tokenized'].items()}, axis=1)

        self.df[target] = self.df[target].apply(lambda x: x.lower().strip())
        if le is None:
            self.le = LabelEncoder()
            self.le.fit(self.df[target])
        else:
            self.le = le
        self.df['label'] = self.le.transform(self.df[target].values)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        if self.test:
            return self.df.iloc[idx]['tokenized']
        return self.df.iloc[idx]['tokenized'], self.df.iloc[idx]['label']

class Classifier(pl.LightningModule):
    def __init__(self,num_classes=5):
        super().__init__()
        self.model = AutoModel.from_pretrained('distilbert-base-multilingual-cased')
        self.classifier = torch.nn.Linear(768, num_classes)
        self.preds = np.array([])
        self.labels = np.array([])
        self.train_acc = torchmetrics.Accuracy(num_classes=num_classes, average='macro', task="multiclass")
        self.valid_acc = torchmetrics.Accuracy(num_classes=num_classes, average='macro', task="multiclass")

    def forward(self, x):
        x = self.model(**x)
        x = x.last_hidden_state[:,0]
        x = self.classifier(x)
        x = torch.softmax(x, dim=1)
        return x

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss_fn = torch.nn.CrossEntropyLoss()
        loss = loss_fn(y_hat, y)
        self.log('train_acc', self.train_acc(y_hat.argmax(dim=1), y), prog_bar = True)
        self.log('train_loss', loss, prog_bar = True)
        return loss

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss_fn = torch.nn.CrossEntropyLoss()
        loss = loss_fn(y_hat, y)
        self.log('test_acc', self.valid_acc(y_hat.argmax(dim=1), y), prog_bar = True)
        self.log('test_loss', loss, prog_bar = True)
        self.preds = np.append(self.preds, y_hat.argmax(dim=1).cpu().numpy())
        self.labels = np.append(self.labels, y.cpu().numpy())
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss_fn = torch.nn.CrossEntropyLoss()
        loss = loss_fn(y_hat, y)
        self.log('val_acc', self.valid_acc(y_hat.argmax(dim=1), y), prog_bar = True)
        self.log('val_loss', loss, prog_bar = True)
        self.preds = np.append(self.preds, y_hat.argmax(dim=1).cpu().numpy())
        self.labels = np.append(self.labels, y.cpu().numpy())
        return loss

    def on_validation_epoch_end(self):
        self.log('val_recall', recall_score(self.labels, self.preds, average='macro'))
        self.preds = np.array([])
        self.labels = np.array([])

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=5e-5)
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': torch.optim.lr_scheduler.LinearLR(optimizer, 0.01, 1,total_iters=100),
                'interval': 'step',
            },
            'monitor': 'val_recall',
            'interval': 'epoch'
        }