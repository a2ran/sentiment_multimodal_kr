# -*- coding: utf-8 -*-
"""speech_audio_classification.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18oknzhVyHjJDSIegLSNhRX5KcPCnQcYz
"""

import pytorch_lightning as pl
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model
from scipy.stats import spearmanr
import torchmetrics
from pytorch_lightning.callbacks import ModelCheckpoint
from sklearn.metrics import recall_score
from sklearn.preprocessing import LabelEncoder
import librosa
import pickle

tqdm.pandas()

import os

class RequestsDataset(torch.utils.data.Dataset):
    @classmethod
    def get_le(cls,df,target='상황'):
        df[target] = df[target].apply(lambda x: x.lower().strip())
        le = LabelEncoder()
        le.fit(df[target])
        return le

    def get_labels(self):
        return self.labels

    def __init__(self,df, data_path,target='상황',max_sec=10,sr=16000, le = None,truncate=True,test=False):
        self.test = test
        self.truncate = truncate
        self.files = df['wav_id'].apply(lambda x: os.path.join(data_path, f'{x}.wav')).copy()

        df[target] = df[target].apply(lambda x: x.lower().strip())
        if le is None:
            self.le = LabelEncoder()
            self.labels = self.le.fit_transform(df[target].values)
        else:
            self.le = le
            self.labels = self.le.transform(df[target].values)
        self.maxlen = max_sec * sr
        print('Loading and processing audio')
        self.processor = Wav2Vec2FeatureExtractor.from_pretrained('kresnik/wav2vec2-large-xlsr-korean')

        self.audio_files = []
        for file_path in self.files:
            if os.path.exists(file_path):
                audio = librosa.load(file_path,sr=sr)[0]
                audio_processed = self.processor(audio, sampling_rate=sr, return_tensors="pt", padding=True).input_values.squeeze(0)
                self.audio_files.append(audio_processed)
            else:
                print(f"File {file_path} does not exist.")
        self.files['audio'] = self.audio_files


    def __len__(self):
        return len(self.files['audio'])

    def __getitem__(self, idx):
        audio = self.files['audio'][idx]
        if not self.truncate:
            return audio, self.labels[idx]
        if (audio.shape[0] > self.maxlen):
            start = np.random.randint(audio.shape[0] - self.maxlen)
            audio = audio[start:start+self.maxlen]
        else:
            audio = torch.cat((audio, torch.zeros(self.maxlen - audio.shape[0])))
        if not self.test:
            return audio, self.labels[idx]
        else:
            return audio

class AudioModel(pl.LightningModule):
    def __init__(self,num_classes, ckpt='kresnik/wav2vec2-large-xlsr-korean'):
        super().__init__()
        self.model = Wav2Vec2Model.from_pretrained(ckpt)
        self.model.feature_extractor._freeze_parameters()
        self.layer_weights = torch.nn.Parameter(torch.ones(25))
        self.linear = torch.nn.Linear(1024*2, num_classes)
        self.dropout = torch.nn.Dropout(0.2)
        self.preds = []
        self.labels = []

    def compute_features(self, x):
        x = self.model(input_values=x, output_hidden_states=True).hidden_states
        x = torch.stack(x,dim=1)
        weights = torch.nn.functional.softmax(self.layer_weights, dim=-1)
        mean_x = x.mean(dim = 2)
        std_x = x.std(dim = 2)
        x = torch.cat((mean_x, std_x), dim=-1)
        x = (x * weights.view(-1,25,1)).sum(dim=1)
        return x

    def forward(self, x):
        x = self.compute_features(x)
        x = self.dropout(x)
        x = self.linear(x)
        x = torch.softmax(x,dim=-1)
        return x

    def training_step(self, batch,batch_idx):
        x,y = batch
        logits = self.forward(x)
        loss_fn = torch.nn.CrossEntropyLoss()
        loss = loss_fn(logits,y)
        self.log('train_loss', loss,sync_dist=True)
        return loss
    
    def validation_step(self, batch, batch_idx):
        x,y = batch
        logits = self.forward(x)
        loss_fn = torch.nn.CrossEntropyLoss()
        loss = loss_fn(logits,y)
        self.log('val_loss', loss,sync_dist=True)
        logits = torch.sigmoid(logits)
        preds = logits.argmax(dim=-1).detach().cpu().numpy()
        self.preds.append(preds)
        self.labels.append(y.detach().cpu().numpy())
        return loss
    
    def on_validation_epoch_end(self):
        self.preds = np.concatenate(self.preds)
        self.labels = np.concatenate(self.labels)
        self.log('val_recall', recall_score(self.labels,self.preds,average='macro'), sync_dist=True)
        self.preds = []
        self.labels = []

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=5e-5)
        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': torch.optim.lr_scheduler.LinearLR(optimizer, 0.01, 1,total_iters=100),
                'interval': 'step',
            },
            'monitor': 'val_recall',
            'interval': 'epoch'
        }